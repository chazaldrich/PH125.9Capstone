---
title: "Movie Recommendation Model"
output:
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Overview

The purpose of this machine learning project was to analyze a collection of move ratings provided by the MovieLens package (https://movielens.org/) and design a recommender model to accurately predict a user’s rating of a given movie.   A 10 million observation MovieLens dataset was selected and split into a train and test dataset.    Final model was then executed against a provided validation dataset to determine a root mean square error rating (RMSE).  Generally,  if the RMSE error is greater than 1,  then the model prediction will be off by more than one star.  The rating is on a 5 star scale.

## Analysis

The final model incorporated the following effects or biases as attributes in the overall model:

Simple average 

The model begins with a simple average of all movies for all users in the dataset and adjusts from there.

Movie Effect 

Some movies are rated differently based on its content, genre, etc.  This effect recognizes the individuality of the movie and calculates a least square estimate using the individual movie’s (moveid from the movielens dataset) average.

User Effect 

Similarly, individual users have tendencies,  maybe an attitude towards one movie or a whole movie type.  The model adds a least square estimate using each userid’s average rating.  The idea here is to categorize users based on their judgements.  Do they tend to be critical with lower star ratings or lenient with higher star ratings across the board?

Regularized Move + User Effect 

Realizing the model is not accounting for variability due to the number of occurrences of both movie and user effects,  we use a penalized least squares Regularization adjustment.   Essentially, the model introduces a ‘penalty’ which tunes out movie ratings with very small populations of ratings. (i.e.  a movie with one user rating should not indicate the same reliability of a movie with 1000 ratings)  As the dataset occurrences  of user ratings for a given movie increases,  the penalty reduces.  To optimize such a regularization of data,  a tuning parameter, lamda, is calculated.   Cross-validation is used to select the point where lamda provides the minimum RMSE.

## Plot of Lambda vs RMSE
```{r echo=FALSE}

#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

## Begin Model 

## RMSE function

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

# create test and train set from edx
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
# ensure users/movies not in the training set are not included in the test set
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# average model

mu_hat <- mean(train_set$rating)
#mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
#naive_rmse

# store results of each model
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

# movie effect model 

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_1_rmse))
# rmse_results 

# user effect model

user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)


model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse))
# rmse_results

#Regularization (penalized least squares)
  
lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse))
# rmse_results 

#  find optimal lambda

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses) 

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
#rmse_results %>% knitr::kable()

```

## Validation

```{r echo=FALSE}

# Execute full model against Validation dataset and add result to rmse_results

# Make sure userId and movieId in validation set are also in edx set

validation  <- validation %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

lambdas_val <- seq(0, 10, 0.25)

rmses_val <- sapply(lambdas_val, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})

#plot lambda for visual of minimum RMSE point

#qplot(lambdas_val, rmses_val)  

lambda_val <- lambdas_val[which.min(rmses)]
#lambda_val

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model Validation Run",  
                                     RMSE = min(rmses_val)))
rmse_results %>% knitr::kable()
```


## Conclusion

The RMSE score for the validation run is:

```{r echo=FALSE}
min(rmses_val)
```


The model’s best result was using regularization with a movie and user effect model.  Regularization provided a small decrease in RMSE.  The largest decrease in RMSE occurred when taking into account the user effect in combination with the movie effect.    In observing remaining data columns to look for other data attributes to consider in the model,   we know no more on a user.  We do know more on the movies:  their genre.  Any deeper analysis to further improve/lower RMSE should look to include a genre effect.